{"cells":[{"metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D89B0EAB542642F98900CCC4D4E49360","scrolled":false,"notebookId":"6371c72700125077ce706c07"},"cell_type":"markdown","source":"# Lecture 10: Model comparation\n\n## Instructor： 胡传鹏（博士）[Dr. Hu Chuan-Peng]\n\n### 南京师范大学心理学院[School of Psychology, Nanjing Normal University]\n"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport arviz as az\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nimport theano.tensor as tt","metadata":{"id":"180DED04984E420D8DE2E055D113EBEA","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"WARNING (theano.link.c.cmodule): install mkl with `conda install mkl-service`: No module named 'mkl'\n"}],"execution_count":1},{"cell_type":"code","source":"data = pd.read_csv('/home/mw/input/data7447/cavanagh_theta_nn.csv') # 载入数据\ndata.head(5) # 查看前5行数据","metadata":{"id":"EB6236DAF4364ED6988BFDA6C6E4731D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":true,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"   subj_idx stim    rt  response     theta  dbs conf\n0         0   LL  1.21       1.0  0.656275    1   HC\n1         0   WL  1.63       1.0 -0.327889    1   LC\n2         0   WW  1.03       1.0 -0.480285    1   HC\n3         0   WL  2.77       1.0  1.927427    1   LC\n4         0   WW  1.14       0.0 -0.213236    1   HC","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subj_idx</th>\n      <th>stim</th>\n      <th>rt</th>\n      <th>response</th>\n      <th>theta</th>\n      <th>dbs</th>\n      <th>conf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>LL</td>\n      <td>1.21</td>\n      <td>1.0</td>\n      <td>0.656275</td>\n      <td>1</td>\n      <td>HC</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>WL</td>\n      <td>1.63</td>\n      <td>1.0</td>\n      <td>-0.327889</td>\n      <td>1</td>\n      <td>LC</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>WW</td>\n      <td>1.03</td>\n      <td>1.0</td>\n      <td>-0.480285</td>\n      <td>1</td>\n      <td>HC</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>WL</td>\n      <td>2.77</td>\n      <td>1.0</td>\n      <td>1.927427</td>\n      <td>1</td>\n      <td>LC</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>WW</td>\n      <td>1.14</td>\n      <td>0.0</td>\n      <td>-0.213236</td>\n      <td>1</td>\n      <td>HC</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":2}],"execution_count":2},{"cell_type":"code","source":"# 调用pandas对象自带的方法，将数据按照‘conf’条件进行分组，选择每组的反应时间，展示描述性统计的内容\ndata.groupby('conf').rt.describe() ","metadata":{"id":"F108F2B40F2044458884849B35672501","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"       count      mean       std    min      25%    50%     75%   max\nconf                                                                 \nHC    1972.0  1.454450  0.706737  0.409  0.92975  1.295  1.8425  4.84\nLC    2016.0  1.351375  0.645456  0.402  0.87775  1.200  1.6900  4.59","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>conf</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>HC</th>\n      <td>1972.0</td>\n      <td>1.454450</td>\n      <td>0.706737</td>\n      <td>0.409</td>\n      <td>0.92975</td>\n      <td>1.295</td>\n      <td>1.8425</td>\n      <td>4.84</td>\n    </tr>\n    <tr>\n      <th>LC</th>\n      <td>2016.0</td>\n      <td>1.351375</td>\n      <td>0.645456</td>\n      <td>0.402</td>\n      <td>0.87775</td>\n      <td>1.200</td>\n      <td>1.6900</td>\n      <td>4.59</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":3}],"execution_count":3},{"cell_type":"code","source":"# 调用pandas对象自带的画图功能，选择每组的反应时间，绘制概率密度函数图\ndata.rt.plot.density() ","metadata":{"id":"ECD0D587538146E590F20C9CD4C83390","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:ylabel='Density'>"},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/ECD0D587538146E590F20C9CD4C83390/rlbor9pfxg.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":4},{"cell_type":"code","source":"# 调用pandas对象自带的画图功能，按照‘conf’进行分组，选择每组的反应时间，绘制概率密度函数图\ndata.groupby(['conf']).rt.plot.density()","metadata":{"id":"A535064986504AAC999A0168FCE6DD6C","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"conf\nHC    AxesSubplot(0.125,0.125;0.775x0.755)\nLC    AxesSubplot(0.125,0.125;0.775x0.755)\nName: rt, dtype: object"},"metadata":{},"execution_count":5},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/A535064986504AAC999A0168FCE6DD6C/rlbor9bj54.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":5},{"cell_type":"code","source":"# 将‘conf’条件进行0， 1编码，HC转化为1，LC转化为0，以便在pymc3中进行计算\ndata.conf = data.conf.map({'HC':1,'LC':0})","metadata":{"id":"12A3713F9D63424D8E3F444703C04B24","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### Model1: Normal\n\n首先，我们假定反应时是呈正态分布的，正态分布的均值是由高一致性条件和低一致性条件决定的。\n\n\n![Image Name](https://docs.pymc.io/en/v3/_images/continuous-18.png)\n\n图片来源：https://docs.pymc.io/en/v3/api/distributions/continuous.html#pymc3.distributions.continuous.Normal","metadata":{"id":"F927A94BBCD540508F38A653E8CA8EA9","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"code","source":"with pm.Model() as NormalModel:\n    # 先验分布: alpha, beta, sigma这三个参数是随机变量\n    sigma = pm.HalfNormal('sigma', sd=1)\n    alpha = pm.Normal('alpha', mu=0, sd=1)\n    beta = pm.Normal('beta', mu=0, sd=1)\n    # 自变量conf是之前已经载入的数据\n    x = pm.Data(\"x\", data['conf'])\n    # 正态分布均值是确定性随机变量，这个变量的值完全由右端值确定\n    mu = pm.Deterministic(\"mu\", alpha + beta*x) \n    # Y的观测值，这是一个特殊的观测随机变量，表示模型数据的可能性。也可以表示模型的似然，通过 observed 参数来告诉这个变量其值是已经被观测到了的，不会被拟合算法改变\n    y_obs = pm.Normal('y_obs',mu=mu,sd=sigma,observed=data['rt'] )","metadata":{"id":"96E1B6A51A5E4995AF3E0D391A1E6978","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 展示模型结构\npm.model_to_graphviz(NormalModel)","metadata":{"id":"0BDF76D3D68F4C78A21B3361B98C364E","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<graphviz.graphs.Digraph at 0x7f614462b250>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/0BDF76D3D68F4C78A21B3361B98C364E/rlborcqeaf.svg\">"},"metadata":{},"execution_count":8}],"execution_count":8},{"cell_type":"code","source":"with NormalModel:\n    trace_for_comp = pm.sample(draws = 2000, tune=1000, target_accept=0.9,chains=2, cores= 2)    \n    # 将pymc的采样对象转化为inferencedata\n    trace=az.from_pymc3(trace_for_comp)","metadata":{"id":"EC6B63389E8D4AFB8CA5450DE5D8D4A9","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n  \nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [beta, alpha, sigma]\n"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 10 seconds.\n"}],"execution_count":9},{"cell_type":"code","source":"# 绘制各参数的采样情况\naz.plot_trace(trace,var_names=['alpha','beta','sigma','mu'])","metadata":{"id":"72F6BC0AFD5E4BFBBE90BE84126D8A3B","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([[<AxesSubplot:title={'center':'alpha'}>,\n        <AxesSubplot:title={'center':'alpha'}>],\n       [<AxesSubplot:title={'center':'beta'}>,\n        <AxesSubplot:title={'center':'beta'}>],\n       [<AxesSubplot:title={'center':'sigma'}>,\n        <AxesSubplot:title={'center':'sigma'}>],\n       [<AxesSubplot:title={'center':'mu'}>,\n        <AxesSubplot:title={'center':'mu'}>]], dtype=object)"},"metadata":{},"execution_count":10},{"output_type":"display_data","data":{"text/plain":"<Figure size 864x576 with 8 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/72F6BC0AFD5E4BFBBE90BE84126D8A3B/rlboyxji9c.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":10},{"cell_type":"code","source":"# 参数的统计值\naz.summary(trace,var_names=['alpha','beta','sigma'])","metadata":{"id":"0950B0FA00DB48DD98BE536B08BA8DBB","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"        mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\nalpha  1.351  0.015   1.325    1.382        0.0      0.0    2500.0    2038.0   \nbeta   0.103  0.021   0.065    0.143        0.0      0.0    2095.0    2378.0   \nsigma  0.677  0.008   0.662    0.690        0.0      0.0    2174.0    1913.0   \n\n       r_hat  \nalpha    1.0  \nbeta     1.0  \nsigma    1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alpha</th>\n      <td>1.351</td>\n      <td>0.015</td>\n      <td>1.325</td>\n      <td>1.382</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2500.0</td>\n      <td>2038.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>beta</th>\n      <td>0.103</td>\n      <td>0.021</td>\n      <td>0.065</td>\n      <td>0.143</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2095.0</td>\n      <td>2378.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>sigma</th>\n      <td>0.677</td>\n      <td>0.008</td>\n      <td>0.662</td>\n      <td>0.690</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2174.0</td>\n      <td>1913.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":11}],"execution_count":11},{"cell_type":"code","source":"with NormalModel:\n     # pm.sample_posterior_predictive()利用trace.posterior的后验分布计算后验预测分布\n    ppc_y = pm.sample_posterior_predictive(trace.posterior) \n#将ppc_y转化为InferenceData对象合并到trace中\naz.concat(trace, az.from_pymc3(posterior_predictive=ppc_y), inplace=True)","metadata":{"id":"59EAD4BE19CC48CD86F069F10D8823BD","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"update_display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [4000/4000 00:06&lt;00:00]\n    </div>\n    "},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/arviz/data/io_pymc3.py:100: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  FutureWarning,\n"}],"execution_count":12},{"cell_type":"code","source":"# 绘制后验预测分布\naz.plot_ppc(trace)","metadata":{"id":"735E869AB9864E0887F4304F6CE4C614","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='y_obs'>"},"metadata":{},"execution_count":13},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/pylabtools.py:151: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/735E869AB9864E0887F4304F6CE4C614/rlbozyoll2.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":13},{"cell_type":"markdown","source":"#### Model2: Log-normal\n\n然后，我们假定反应时是呈log正态分布的，LogNormal分布的均值是由高一致性条件和低一致性条件决定的。\n\n\n![Image Name](https://docs.pymc.io/en/v3/_images/continuous-16.png)\n\n\n\n图片来源：https://docs.pymc.io/en/v3/api/distributions/continuous.html#pymc3.distributions.continuous.LogitNormal","metadata":{"id":"D7A33179EAC94F968EBEDCE18B9F7B54","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"code","source":"with pm.Model() as LogNormal:\n    # 先验分布: alpha, beta, sigma这三个参数是随机变量\n    sigma = pm.HalfNormal('sigma', sd=1)\n    alpha = pm.Normal('alpha', mu=0, sd=1)\n    beta = pm.Normal('beta', mu=0, sd=1)\n    # 自变量conf是之前已经载入的数据\n    x = pm.Data(\"x\", data['conf'])\n    # 正态分布均值是确定性随机变量，这个变量的值完全由右端值确定\n    mu = pm.Deterministic(\"mu\", alpha + beta*x) \n    # Y的观测值，这是一个特殊的观测随机变量，表示模型数据的可能性。也可以表示模型的似然，通过 observed 参数来告诉这个变量其值是已经被观测到了的，不会被拟合算法改变\n    y_obs = pm.Lognormal('y_obs',mu=mu,sd=sigma,observed=data['rt'] )","metadata":{"id":"55D72B3FF6274317829D74EC8C4C5F99","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# 展示模型结构\npm.model_to_graphviz(LogNormal)","metadata":{"id":"30736F5A3C6B499282B9CC9D5426D679","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<graphviz.graphs.Digraph at 0x7f5fd0475850>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/30736F5A3C6B499282B9CC9D5426D679/rlbozyhfhs.svg\">"},"metadata":{},"execution_count":15}],"execution_count":15},{"cell_type":"code","source":"with LogNormal:\n    trace2_for_comp = pm.sample(draws = 2000, tune=1000, target_accept=0.9,chains=2, cores= 2)\n    # 将pymc的采样对象转化为inferencedata    \n    trace2=az.from_pymc3(trace2_for_comp)","metadata":{"id":"4461A25C102D41CB87894A78FACFE2F2","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n  \nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [beta, alpha, sigma]\n"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 6 seconds.\n"}],"execution_count":16},{"cell_type":"code","source":"# 绘制各参数的采样情况\naz.plot_trace(trace2,var_names=['alpha','beta','sigma','mu'])","metadata":{"id":"C37FA075D48842BB8EF5C3ED450B8954","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([[<AxesSubplot:title={'center':'alpha'}>,\n        <AxesSubplot:title={'center':'alpha'}>],\n       [<AxesSubplot:title={'center':'beta'}>,\n        <AxesSubplot:title={'center':'beta'}>],\n       [<AxesSubplot:title={'center':'sigma'}>,\n        <AxesSubplot:title={'center':'sigma'}>],\n       [<AxesSubplot:title={'center':'mu'}>,\n        <AxesSubplot:title={'center':'mu'}>]], dtype=object)"},"metadata":{},"execution_count":17},{"output_type":"display_data","data":{"text/plain":"<Figure size 864x576 with 8 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/C37FA075D48842BB8EF5C3ED450B8954/rlbp7f93c7.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":17},{"cell_type":"code","source":"# 参数的统计值\naz.summary(trace2,var_names=['alpha','beta','sigma'])","metadata":{"id":"6DFF8D88AFE64DEE8382C5BA63734C7E","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"        mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\nalpha  0.197  0.011   0.176    0.216        0.0      0.0    2060.0    2092.0   \nbeta   0.066  0.015   0.037    0.094        0.0      0.0    2110.0    1974.0   \nsigma  0.465  0.005   0.455    0.474        0.0      0.0    2335.0    2134.0   \n\n       r_hat  \nalpha    1.0  \nbeta     1.0  \nsigma    1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alpha</th>\n      <td>0.197</td>\n      <td>0.011</td>\n      <td>0.176</td>\n      <td>0.216</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2060.0</td>\n      <td>2092.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>beta</th>\n      <td>0.066</td>\n      <td>0.015</td>\n      <td>0.037</td>\n      <td>0.094</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2110.0</td>\n      <td>1974.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>sigma</th>\n      <td>0.465</td>\n      <td>0.005</td>\n      <td>0.455</td>\n      <td>0.474</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2335.0</td>\n      <td>2134.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":18}],"execution_count":18},{"cell_type":"code","source":"with LogNormal:\n    # pm.sample_posterior_predictive()利用trace.posterior的后验分布计算后验预测分布\n    ppc_y = pm.sample_posterior_predictive(trace2.posterior) \n#将ppc_y转化为InferenceData对象合并到trace中\naz.concat(trace2, az.from_pymc3(posterior_predictive=ppc_y), inplace=True)","metadata":{"id":"2E69A3EAF68846CD95EC6C15E5C4BFB8","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"update_display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [4000/4000 00:05&lt;00:00]\n    </div>\n    "},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/arviz/data/io_pymc3.py:100: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  FutureWarning,\n"}],"execution_count":19},{"cell_type":"code","source":"# 绘制后验预测分布\naz.plot_ppc(trace2)","metadata":{"id":"84AA12B197AE455D8BF40FD167BF84AA","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='y_obs'>"},"metadata":{},"execution_count":20},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/84AA12B197AE455D8BF40FD167BF84AA/rlbp8d4a69.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":20},{"cell_type":"markdown","source":"#### Model3: Gamma\n\n我们假定反应时是呈gamma分布的，gamma分布的alpha是由高一致性条件和低一致性条件决定的。\n\n\n![Image Name](https://docs.pymc.io/en/v3/_images/continuous-6.png)\n\n图片来源：https://docs.pymc.io/en/v3/api/distributions/continuous.html#pymc3.distributions.continuous.Gamma\n","metadata":{"id":"676785E87AB84CCA8713AB6F1AAC6255","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"code","source":"with pm.Model() as Gamma:\n    # 先验分布:intercept, theta, beta这三个参数是随机变量\n    intercept = pm.HalfNormal('intercept', sd=1)\n    theta = pm.HalfNormal('theta', sd=1)\n    beta = pm.HalfNormal('beta', sd=1)   \n    # 自变量conf是之前已经载入的数据\n    x = pm.Data(\"x\", data['conf'])\n    # 参数alpha是确定性随机变量，这个变量的值完全由右端值确定\n    alpha = pm.Deterministic(\"alpha\",  intercept+ theta*x) \n    # Y的观测值，这是一个特殊的观测随机变量，表示模型数据的可能性。也可以表示模型的似然，通过 observed 参数来告诉这个变量其值是已经被观测到了的，不会被拟合算法改变\n    y_obs = pm.Gamma('y_obs',alpha=alpha,beta=beta,observed=data['rt'] )","metadata":{"id":"0120ACDED2BA4D2F8631B14B2A512624","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# 展示模型结构\npm.model_to_graphviz(Gamma)","metadata":{"id":"BE80C3787B36411C8A0B2893446C57B7","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<graphviz.graphs.Digraph at 0x7f5f969f6910>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/BE80C3787B36411C8A0B2893446C57B7/rlbp8e1s2n.svg\">"},"metadata":{},"execution_count":22}],"execution_count":22},{"cell_type":"code","source":"with Gamma:\n    trace3_for_comp = pm.sample(draws = 2000, tune=1000, target_accept=0.9,chains=2, cores= 2)    \n    # 将pymc的采样对象转化为inferencedata\n    trace3=az.from_pymc3(trace3_for_comp)","metadata":{"id":"595F516FDCAB4A3C821F09CE6B8FE28D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n  \nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 2 jobs)\nNUTS: [beta, theta, intercept]\n"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 26 seconds.\n"}],"execution_count":23},{"cell_type":"code","source":"# 绘制各参数的采样情况\naz.plot_trace(trace3,var_names=['beta','intercept','theta'])","metadata":{"id":"7D89712978D347B98A92A0F9ACF7D28D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([[<AxesSubplot:title={'center':'beta'}>,\n        <AxesSubplot:title={'center':'beta'}>],\n       [<AxesSubplot:title={'center':'intercept'}>,\n        <AxesSubplot:title={'center':'intercept'}>],\n       [<AxesSubplot:title={'center':'theta'}>,\n        <AxesSubplot:title={'center':'theta'}>]], dtype=object)"},"metadata":{},"execution_count":24},{"output_type":"display_data","data":{"text/plain":"<Figure size 864x432 with 6 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/7D89712978D347B98A92A0F9ACF7D28D/rlbp989387.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":24},{"cell_type":"code","source":"# 参数的统计值\naz.summary(trace3,var_names=['beta','intercept','theta'])","metadata":{"id":"0F6385F002B14CA9805B85AB20D4F94F","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"            mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\nbeta       3.355  0.075   3.214    3.498      0.002    0.001    1525.0   \nintercept  4.566  0.102   4.372    4.760      0.003    0.002    1417.0   \ntheta      0.287  0.063   0.169    0.403      0.002    0.001    1523.0   \n\n           ess_tail  r_hat  \nbeta         1713.0    1.0  \nintercept    1466.0    1.0  \ntheta        1317.0    1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>beta</th>\n      <td>3.355</td>\n      <td>0.075</td>\n      <td>3.214</td>\n      <td>3.498</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>1525.0</td>\n      <td>1713.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>intercept</th>\n      <td>4.566</td>\n      <td>0.102</td>\n      <td>4.372</td>\n      <td>4.760</td>\n      <td>0.003</td>\n      <td>0.002</td>\n      <td>1417.0</td>\n      <td>1466.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>theta</th>\n      <td>0.287</td>\n      <td>0.063</td>\n      <td>0.169</td>\n      <td>0.403</td>\n      <td>0.002</td>\n      <td>0.001</td>\n      <td>1523.0</td>\n      <td>1317.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":25}],"execution_count":25},{"cell_type":"code","source":"with Gamma:\n    # pm.sample_posterior_predictive()利用trace.posterior的后验分布计算后验预测分布\n    ppc_y = pm.sample_posterior_predictive(trace3.posterior) \n#将ppc_y转化为InferenceData对象合并到trace中\naz.concat(trace3, az.from_pymc3(posterior_predictive=ppc_y), inplace=True)","metadata":{"id":"AE83F205FC3C45C782E4767118C39CA4","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"update_display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='4000' class='' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [4000/4000 00:06&lt;00:00]\n    </div>\n    "},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/arviz/data/io_pymc3.py:100: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  FutureWarning,\n"}],"execution_count":26},{"cell_type":"code","source":"# 绘制后验预测分布\naz.plot_ppc(trace3)","metadata":{"id":"4C65964DA6FC4648819B8327252C3BC8","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='y_obs'>"},"metadata":{},"execution_count":27},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/4C65964DA6FC4648819B8327252C3BC8/rlbpa8nlnl.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":27},{"cell_type":"markdown","source":"#### Model4: ex-Gaussian\n\n我们假定反应时是混合高斯分布\n\n![Image Name](https://docs.pymc.io/en/latest/_images/pymc-ExGaussian-1.png)\n\n图片来源：https://docs.pymc.io/en/latest/api/distributions/generated/pymc.ExGaussian.html","metadata":{"id":"D430889E48AC448B9DA40FE3899E3E88","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"code","source":"k=3\nndata=3988\nwith pm.Model() as exgaussian:\n    # 聚类数量\n    p = pm.Dirichlet(\"p\", a=np.array([1.0, 1.0, 1.0]), shape=k)\n    # 确保所有的聚类都有一些数量点\n    p_min_potential = pm.Potential(\"p_min_potential\", tt.switch(tt.min(p) < 0.1, -np.inf, 0))\n    # 聚类中心\n    means = pm.Normal(\"means\", mu=[0, 0, 0], sigma=1, shape=k)\n    # break symmetry\n    order_means_potential = pm.Potential(\n        \"order_means_potential\",\n        tt.switch(means[1] - means[0] < 0, -np.inf, 0)\n        + tt.switch(means[2] - means[1] < 0, -np.inf, 0),\n    )\n    # 测量误差\n    sd = pm.Uniform(\"sd\", lower=0, upper=3)\n    # 每一个观测值的潜在分类\n    category = pm.Categorical(\"category\", p=p, shape=ndata)\n    # 每一个观测值的似然\n    points = pm.Normal(\"obs\", mu=means[category], sigma=sd, observed=data['rt'])","metadata":{"id":"9E9459FA34A04E1BACE6D3FDB75DA8A0","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"with exgaussian:\n    # 采样过程\n    step1 = pm.Metropolis(vars=[p, sd, means])\n    step2 = pm.ElemwiseCategorical(vars=[category], values=[0, 1, 2])\n    tr_for_comp = pm.sample(100, step=[step1, step2], tune=50)\n    # 将pymc的采样对象转化为inferencedata\n    tr=az.from_pymc3(tr_for_comp)","metadata":{"id":"E1B4C2497B48411886CC56BA73EE60EF","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: ElemwiseCategorical is deprecated, switch to CategoricalGibbsMetropolis.\n  after removing the cwd from sys.path.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n  \"\"\"\nOnly 100 samples in chain.\nMultiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n>CompoundStep\n>>Metropolis: [means]\n>>Metropolis: [sd]\n>>Metropolis: [p]\n>ElemwiseCategorical: [category]\n"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pymc3/step_methods/metropolis.py:226: RuntimeWarning: overflow encountered in exp\n  \"accept\": np.exp(accept),\n/opt/conda/lib/python3.7/site-packages/pymc3/step_methods/metropolis.py:226: RuntimeWarning: overflow encountered in exp\n  \"accept\": np.exp(accept),\nSampling 4 chains for 50 tune and 100 draw iterations (200 + 400 draws total) took 27 seconds.\nThe rhat statistic is larger than 1.4 for some parameters. The sampler did not converge.\nThe number of effective samples is smaller than 10% for some parameters.\n"}],"execution_count":29},{"cell_type":"code","source":"az.plot_trace(tr, var_names=[\"p\", \"sd\", \"means\"]);","metadata":{"id":"28E3EFD0347D47FB8BFA596B34062EF8","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:620: RuntimeWarning: invalid value encountered in double_scalars\n  x_std = (((x ** 2).sum() / x_len) - (x.sum() / x_len) ** 2) ** 0.5\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:248: RuntimeWarning: divide by zero encountered in double_scalars\n  t_j = np.power((c1 * (c2 / (N * f))), (2.0 / (3.0 + 2.0 * j)))\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:252: RuntimeWarning: divide by zero encountered in double_scalars\n  out = t - (2 * N * np.pi ** 0.5 * f) ** (-0.4)\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:276: RuntimeWarning: invalid value encountered in double_scalars\n  bw = (_bw_silverman(x) / np.ptp(x)) ** 2\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:760: RuntimeWarning: invalid value encountered in true_divide\n  f = grid_counts / bin_width / len(x)\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:770: UserWarning: Something failed when estimating the bandwidth. Please check your data\n  warnings.warn(\"Something failed when estimating the bandwidth. Please check your data\")\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:620: RuntimeWarning: invalid value encountered in double_scalars\n  x_std = (((x ** 2).sum() / x_len) - (x.sum() / x_len) ** 2) ** 0.5\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:248: RuntimeWarning: divide by zero encountered in double_scalars\n  t_j = np.power((c1 * (c2 / (N * f))), (2.0 / (3.0 + 2.0 * j)))\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:252: RuntimeWarning: divide by zero encountered in double_scalars\n  out = t - (2 * N * np.pi ** 0.5 * f) ** (-0.4)\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:276: RuntimeWarning: invalid value encountered in double_scalars\n  bw = (_bw_silverman(x) / np.ptp(x)) ** 2\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:760: RuntimeWarning: invalid value encountered in true_divide\n  f = grid_counts / bin_width / len(x)\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:770: UserWarning: Something failed when estimating the bandwidth. Please check your data\n  warnings.warn(\"Something failed when estimating the bandwidth. Please check your data\")\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:620: RuntimeWarning: invalid value encountered in double_scalars\n  x_std = (((x ** 2).sum() / x_len) - (x.sum() / x_len) ** 2) ** 0.5\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:248: RuntimeWarning: divide by zero encountered in double_scalars\n  t_j = np.power((c1 * (c2 / (N * f))), (2.0 / (3.0 + 2.0 * j)))\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:252: RuntimeWarning: divide by zero encountered in double_scalars\n  out = t - (2 * N * np.pi ** 0.5 * f) ** (-0.4)\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:276: RuntimeWarning: invalid value encountered in double_scalars\n  bw = (_bw_silverman(x) / np.ptp(x)) ** 2\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:760: RuntimeWarning: invalid value encountered in true_divide\n  f = grid_counts / bin_width / len(x)\n/opt/conda/lib/python3.7/site-packages/arviz/stats/density_utils.py:770: UserWarning: Something failed when estimating the bandwidth. Please check your data\n  warnings.warn(\"Something failed when estimating the bandwidth. Please check your data\")\n"},{"output_type":"display_data","data":{"text/plain":"<Figure size 864x432 with 6 Axes>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/28E3EFD0347D47FB8BFA596B34062EF8/rlbpbeis0h.png\">"},"metadata":{"needs_background":"light"},"execution_count":null}],"execution_count":30},{"cell_type":"code","source":"pm.model_to_graphviz(exgaussian)","metadata":{"id":"FCDD7CAC435347629B74E8985AFAC393","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"<graphviz.graphs.Digraph at 0x7f5fa4749b50>","text/html":"<img src=\"https://cdn.kesci.com/upload/rt/FCDD7CAC435347629B74E8985AFAC393/rlbpbexi1p.svg\">"},"metadata":{},"execution_count":31}],"execution_count":31},{"cell_type":"code","source":"az.summary(tr,var_names=[\"p\", \"sd\", \"means\"])","metadata":{"id":"A76A657EDC8247488FE38C8C0730B6E3","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":"           mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\np[0]      0.150  0.064   0.103    0.333      0.024    0.018       7.0   \np[1]      0.447  0.131   0.333    0.713      0.061    0.046       5.0   \np[2]      0.403  0.132   0.163    0.539      0.059    0.044       5.0   \nsd        0.743  0.266   0.497    1.522      0.116    0.087       5.0   \nmeans[0]  0.024  0.407  -0.638    0.630      0.182    0.137       5.0   \nmeans[1]  1.018  0.364   0.259    1.428      0.151    0.123       5.0   \nmeans[2]  1.526  0.515   0.588    2.169      0.223    0.181       5.0   \n\n          ess_tail  r_hat  \np[0]           5.0   1.71  \np[1]           7.0   3.10  \np[2]           5.0   3.87  \nsd             8.0   3.58  \nmeans[0]       5.0   2.84  \nmeans[1]      11.0   2.84  \nmeans[2]      11.0   2.66  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>p[0]</th>\n      <td>0.150</td>\n      <td>0.064</td>\n      <td>0.103</td>\n      <td>0.333</td>\n      <td>0.024</td>\n      <td>0.018</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>1.71</td>\n    </tr>\n    <tr>\n      <th>p[1]</th>\n      <td>0.447</td>\n      <td>0.131</td>\n      <td>0.333</td>\n      <td>0.713</td>\n      <td>0.061</td>\n      <td>0.046</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>3.10</td>\n    </tr>\n    <tr>\n      <th>p[2]</th>\n      <td>0.403</td>\n      <td>0.132</td>\n      <td>0.163</td>\n      <td>0.539</td>\n      <td>0.059</td>\n      <td>0.044</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>3.87</td>\n    </tr>\n    <tr>\n      <th>sd</th>\n      <td>0.743</td>\n      <td>0.266</td>\n      <td>0.497</td>\n      <td>1.522</td>\n      <td>0.116</td>\n      <td>0.087</td>\n      <td>5.0</td>\n      <td>8.0</td>\n      <td>3.58</td>\n    </tr>\n    <tr>\n      <th>means[0]</th>\n      <td>0.024</td>\n      <td>0.407</td>\n      <td>-0.638</td>\n      <td>0.630</td>\n      <td>0.182</td>\n      <td>0.137</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>2.84</td>\n    </tr>\n    <tr>\n      <th>means[1]</th>\n      <td>1.018</td>\n      <td>0.364</td>\n      <td>0.259</td>\n      <td>1.428</td>\n      <td>0.151</td>\n      <td>0.123</td>\n      <td>5.0</td>\n      <td>11.0</td>\n      <td>2.84</td>\n    </tr>\n    <tr>\n      <th>means[2]</th>\n      <td>1.526</td>\n      <td>0.515</td>\n      <td>0.588</td>\n      <td>2.169</td>\n      <td>0.223</td>\n      <td>0.181</td>\n      <td>5.0</td>\n      <td>11.0</td>\n      <td>2.66</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":32}],"execution_count":32},{"cell_type":"code","source":"with exgaussian:\n    # pm.sample_posterior_predictive()利用trace.posterior的后验分布计算后验预测分布\n    ppc_y = pm.sample_posterior_predictive(tr.posterior) \n#将ppc_y转化为InferenceData对象合并到trace中\naz.concat(tr, az.from_pymc3(posterior_predictive=ppc_y), inplace=True)","metadata":{"id":"9BF16E902D944343B523FC288712D254","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pymc3/sampling.py:1701: UserWarning: The effect of Potentials on other parameters is ignored during posterior predictive sampling. This is likely to lead to invalid or biased predictive samples.\n  UserWarning,\n"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{},"execution_count":null},{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/arviz/data/io_pymc3.py:100: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n  FutureWarning,\n"}],"execution_count":33},{"cell_type":"code","source":"# 将三个模型的采样结果进行比较\ncompare_dict = {\"normal\": trace, \"log-nomal\": trace2, \"gamma\": trace3,\"exgaussian\":tr}\n# 选择loo方法进行比较\ncomp = az.compare(compare_dict, ic='loo')\ncomp","metadata":{"id":"2E080B5C635C480D81D8D852978C8BDF","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/arviz/stats/stats.py:146: UserWarning: The default method used to estimate the weights for each model,has changed from BB-pseudo-BMA to stacking\n  \"The default method used to estimate the weights for each model,\"\n/opt/conda/lib/python3.7/site-packages/arviz/stats/stats.py:656: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  \"Estimated shape parameter of Pareto distribution is greater than 0.7 for \"\n"},{"output_type":"execute_result","data":{"text/plain":"            rank          loo        p_loo        d_loo  weight         se  \\\nlog-nomal      0 -3520.974461     2.902057     0.000000     1.0  49.747097   \ngamma          1 -3596.136813     2.609188    75.162353     0.0  48.965863   \nnormal         2 -4102.131277     3.625969   581.156816     0.0  58.268329   \nexgaussian     3 -4848.787161  1082.174989  1327.812700     0.0  52.037463   \n\n                  dse  warning loo_scale  \nlog-nomal    0.000000    False       log  \ngamma        8.482589    False       log  \nnormal      29.490654    False       log  \nexgaussian  24.553437     True       log  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rank</th>\n      <th>loo</th>\n      <th>p_loo</th>\n      <th>d_loo</th>\n      <th>weight</th>\n      <th>se</th>\n      <th>dse</th>\n      <th>warning</th>\n      <th>loo_scale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>log-nomal</th>\n      <td>0</td>\n      <td>-3520.974461</td>\n      <td>2.902057</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>49.747097</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>log</td>\n    </tr>\n    <tr>\n      <th>gamma</th>\n      <td>1</td>\n      <td>-3596.136813</td>\n      <td>2.609188</td>\n      <td>75.162353</td>\n      <td>0.0</td>\n      <td>48.965863</td>\n      <td>8.482589</td>\n      <td>False</td>\n      <td>log</td>\n    </tr>\n    <tr>\n      <th>normal</th>\n      <td>2</td>\n      <td>-4102.131277</td>\n      <td>3.625969</td>\n      <td>581.156816</td>\n      <td>0.0</td>\n      <td>58.268329</td>\n      <td>29.490654</td>\n      <td>False</td>\n      <td>log</td>\n    </tr>\n    <tr>\n      <th>exgaussian</th>\n      <td>3</td>\n      <td>-4848.787161</td>\n      <td>1082.174989</td>\n      <td>1327.812700</td>\n      <td>0.0</td>\n      <td>52.037463</td>\n      <td>24.553437</td>\n      <td>True</td>\n      <td>log</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":34}],"execution_count":34},{"cell_type":"code","source":"# 将三个模型的采样结果进行比较\ncompare_dict = {\"normal\": trace, \"log-nomal\": trace2, \"gamma\": trace3,\"exgaussian\":tr}\n# 选择loo方法进行比较\ncomp = az.compare(compare_dict, ic='waic')\ncomp","metadata":{"id":"C18649AC5ED74BC78570B5837D14FBB0","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"output_type":"stream","name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/arviz/stats/stats.py:146: UserWarning: The default method used to estimate the weights for each model,has changed from BB-pseudo-BMA to stacking\n  \"The default method used to estimate the weights for each model,\"\n/opt/conda/lib/python3.7/site-packages/arviz/stats/stats.py:1407: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. \nSee http://arxiv.org/abs/1507.04544 for details\n  \"For one or more samples the posterior variance of the log predictive \"\n"},{"output_type":"execute_result","data":{"text/plain":"            rank         waic       p_waic       d_waic        weight  \\\nlog-nomal      0 -3520.974454     2.902051     0.000000  1.000000e+00   \ngamma          1 -3596.136828     2.609203    75.162373  1.668388e-07   \nnormal         2 -4102.131589     3.626281   581.157135  1.031748e-07   \nexgaussian     3 -4789.531820  1022.919649  1268.557366  0.000000e+00   \n\n                   se        dse  warning waic_scale  \nlog-nomal   49.747097   0.000000    False        log  \ngamma       48.965864   8.482589    False        log  \nnormal      58.268364  29.490686    False        log  \nexgaussian  51.735685  24.327499     True        log  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rank</th>\n      <th>waic</th>\n      <th>p_waic</th>\n      <th>d_waic</th>\n      <th>weight</th>\n      <th>se</th>\n      <th>dse</th>\n      <th>warning</th>\n      <th>waic_scale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>log-nomal</th>\n      <td>0</td>\n      <td>-3520.974454</td>\n      <td>2.902051</td>\n      <td>0.000000</td>\n      <td>1.000000e+00</td>\n      <td>49.747097</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>log</td>\n    </tr>\n    <tr>\n      <th>gamma</th>\n      <td>1</td>\n      <td>-3596.136828</td>\n      <td>2.609203</td>\n      <td>75.162373</td>\n      <td>1.668388e-07</td>\n      <td>48.965864</td>\n      <td>8.482589</td>\n      <td>False</td>\n      <td>log</td>\n    </tr>\n    <tr>\n      <th>normal</th>\n      <td>2</td>\n      <td>-4102.131589</td>\n      <td>3.626281</td>\n      <td>581.157135</td>\n      <td>1.031748e-07</td>\n      <td>58.268364</td>\n      <td>29.490686</td>\n      <td>False</td>\n      <td>log</td>\n    </tr>\n    <tr>\n      <th>exgaussian</th>\n      <td>3</td>\n      <td>-4789.531820</td>\n      <td>1022.919649</td>\n      <td>1268.557366</td>\n      <td>0.000000e+00</td>\n      <td>51.735685</td>\n      <td>24.327499</td>\n      <td>True</td>\n      <td>log</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":35}],"execution_count":35},{"cell_type":"markdown","source":"### 模型的泛化能力：预测准确性 (predictive accuracy)\n\n前面提到的诸多模型拟合优度 (Goodness of fit) 指标，只能衡量模型对于当前样本的拟合程度。\n\n对于样本外的数据，我们不确定该模型是否具有**泛化能力**，即该模型是否能准确的预测样本外的数据。\n\n为了评估模型的**预测准确性 (predictive accuracy)**，我们有以下3种策略：\n1. 通过新数据对模型进行评估。我们可以收集新的数据，并检验模型的预测能力。\n2. 从已有样本中拿出一部分数据用来预测。即交叉验证。\n3. 通过统计方法进行近似。比如使用对交叉验证近似的信息熵指标来评估模型的泛化能力。","metadata":{"id":"D96FB13363D04BFEA522F92E6498C613","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 交叉验证\n\n收集新数据来检验模型的预测能力是一种理所当然的直觉。但心理学数据不同于其他学科的数据，它常受到**时间因素**和**抽样**的影响。\n- 比如，心境可能随着季节变化，因此在不同季节收集到的数据会受到时间的影响。\n\n因此，一种更高效的方法是，一次性多收集一些数据，选择其中的一部分作为预测数据。\n\n但问题在于，我们选择哪一部分数据作为预测数据呐？或者说，我们该如何有效的对数据进行抽取呐？\n\n**交叉验证**的目的就在于：提供不同的抽取预测数据的策略","metadata":{"id":"F97B50FF2C2F4DFF88FFD5FF0ED7923C","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"常见的交叉验证策略：\n1. 分半交叉验证 (Split-half cross-validation)\n\t- 分半交叉验证将观测数据对半分成两部分，分别在不同的数据集上拟合模型，并在另外一半数据集上验证模型，最后再对比不同的模型在两份数据集作为验证集时的预测准确度。\n2. K 折交叉验证 (K-fold cross-validation) \n\t- K 折交叉验证把数据分成 K 分，其中一份作为训练集，其余的 K-1 分数据集作为验证集，总共重复这个流程 K 次。以 K 次验证结果的均值作为验证标准。\n3. 留一法交叉验证 (Leave-one-out cross-validation)\n\t- 留一法交叉验证是 K 折交叉验证的一个特例，当分折的数量等于数据的数量时，K 折留一法便成了留一法交叉验证。留一法交叉验证相较于普通的交叉验证方法，几乎使用了所有数据去训练模型，因此留一法交叉验证的训练模型时的**偏差 (bias) 更小、更鲁棒**，但是又因为验证集只有一个数据点，验证模型的时候**留一法交叉验证的方差 (Variance) 也会更大**。","metadata":{"id":"E457BA594B0F4F80B15A2D2AF3F7422A","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"K 折交叉验证图示\n![](https://hub.packtpub.com/wp-content/uploads/2019/05/KFold.png)\n\nsource: https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/","metadata":{"id":"B67FB3F3C809458182966934D0E6AFED","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 信息准则(information criteria)\n\n尽管交叉验证存在诸多优势，并且避免了收集数据的潜在问题，但是交叉验证**在认知神经科学里的认知建模领域里的使用并不广泛**，\n\n其最主要的原因在于，研究者收集的数据**样本量往往有限**，然而很多计算模型却对数据样本量有所需求，如果使用交叉验证，将数据拆分，那么很有可能导致拟合模型的试次数量不足，使得模型拟合和验证的结果较差，进而导致产生一类错误和二类错误的概率增大。\n\n为了解决这些问题，统计学家提出了一种衡量预测准确性的指标，即**信息准则**(information criteria)。","metadata":{"id":"3B7839B01105470698045D407E6182FC","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"信息准则是对统计模型**预测精度**的一种度量。它考虑**模型与数据的匹配程度**，并通过**模型的复杂性**进行矫正(correction to bias)。\n\n$信息准则 = deviance + correction$\n- deviance 为偏差，反应了模型与数据的匹配程度。可以通过对数似然 log likelihood 进行计算。注意 log likelihood 也可以称为 lpd (log predictive density)。\n- correction 为矫正，与模型的复杂程度相关。模型越复杂时越容易过拟合(overfitting)，因此，矫正项也会越大。\n\n可见，信息准则越小，偏差和矫正就越小。因此，**信息准则越小，代表模型的预测性越好**。","metadata":{"id":"45D2CF4FDBE64D3EAF219D3A4F071EBC","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"常见信息准则有4类：\n1. AIC (Akaike information criterion)\n2. DIC (Deviance information criterion)\n3. WAIC (Widely applicable information criterion)\n4. LOO-CV (Leave-one-out cross-validation)","metadata":{"id":"D2D06FBEEBB44356825B35618BA37F9D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 1. AIC (Akaike information criterion)","metadata":{"id":"F5722DE240CF48DA9617746242C362B9","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"AIC是最简单的信息准则标准，由日本统计学家赤池弘次 (Hirotugu Akaike) 提出 (Akaike, 1974)，是频率主义统计学里最为经典的模型比较指标之一。\n\n其表达式如下：\n$$\nA I C= -2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)+2 p_{A I C}\n$$","metadata":{"id":"1BE5803511E04B51978203FD4FD48EA9","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"$-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$ 为偏差deviance，描述了模型对于当前数据的匹配程度。\n- 其中，$\\hat{\\theta}_{m l e}$为最大似然法求得的参数值。 \n- $\\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$ 为对数似然(log likelihood)，也可称为 lpd (log predictive density)。\n- 偏差deviance为负二倍(-2*)的对数似然值。模型拟合的越好，似然值越大，因此$\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$越大。相应的 $-2\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$ 的值越小。那么 AIC的值也越小。","metadata":{"id":"35A8F7E7902A4996A421006B9D0D01D3","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"$2 p_{A I C}$ 为矫正(correction)。\n- 其中，$p_{A I C}$ 为模型的参数数量。\n- 矫正(correction)为参数数量的两倍，描述了模型的复杂程度。模型越复杂，潜在的参数数量可能越多，那么$p_{A I C}$越大。","metadata":{"id":"647EA98CB8844CD1A719B5634836BD46","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"需要注意的是 AIC 只考虑了最大似然对应的参数值 $\\hat{\\theta}_{m l e}$, 因此它适用于频率学派模型的评估。而对于贝叶斯学派来说，**由于参数为分布，因此不能使用AIC来评估模型**。","metadata":{"id":"9B2BB3727EF2489A8946F31308F3A7C0","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 2. DIC (Deviance information criterion)\n\n为了解决 AIC 无法评估贝叶斯模型。\n\n统计学家们提出了\"贝叶斯参数估计版的 AIC\"，即 DIC (Deviance information criterion) 。\n\n$$\n\\mathrm{DIC} = -2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right) +2 p_{DIC}\n$$","metadata":{"id":"688806BC04BA4778A9ABED44B743F26D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"$-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)$ 为偏差(deviance)，记为D。\n- 可以看到，该偏差与AIC中的偏差$-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$非常相似。\n- 区别在于，AIC 中的 $\\hat{\\theta}_{m l e}$被替换为$\\bar{\\theta}$。因为贝叶斯框架中，参数不是固定值而是概率分布，因此 $\\bar{\\theta}$ 代表的是参数后验分布的均值 $\\bar{\\theta}$。","metadata":{"id":"8CCB7EBC84F74C37A9C5CFB2CDC0FE24","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"![Image Name](https://cdn.kesci.com/upload/image/rl7ys1gvzz.png?imageView2/0/w/640/h/640)","metadata":{"id":"4FB3BBBFE1FA43948D20C7F1D740C360","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"$p_{\\mathrm{DIC}}$ 为矫正(correction). \n\n$p_{\\mathrm{DIC}}= 2(\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)-\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y \\mid \\theta^{s}\\right))$\n- 其中$\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)$为偏差D。\n- $\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y \\mid \\theta^{s}\\right)$为所有参数采样所计算到的对数似然的平均值。参数采样包含s个样本，编号为1到s。","metadata":{"id":"4E6D869A679941BBA6CD91A0218A08D3","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"相比于AIC的矫正项 $p_{\\mathrm{AIC}}$，$p_{\\mathrm{DIC}}$ 是基于数据的矫正(data-based bias correction)。因为 $p_{\\mathrm{DIC}}$ 考虑了数据在不同参数的对数似然下的影响。\n\n$p_{\\mathrm{DIC}}$ 存在另一种表达方式：\n$$\np_{\\mathrm{DIC}}=2\\operatorname{var}(\\log p(y \\mid \\theta))\n$$\n- 其中，var表示计算方差。$\\operatorname{var}(\\log p(y \\mid \\theta))$表示所有参数计算所得到的对数似然的方差。\n- 这种计算方式与第一种计算方式得到的结果一致，并且可以避免 $p_{\\mathrm{DIC}}$ 为负数。","metadata":{"id":"344A16E51696422E8F81B27C5164090B","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"DIC  是心理学领域最常用的模型评估指标之一。\n- 与 AIC 只是简单地使用了参数数量作为复杂度的惩罚项不同的是，DIC **利用了 MCMC 采样的参数后验分布去计算模型的有效参数数量**。\n- 另外 DIC的计算速度与AIC一样很快，这与后面会介绍的其他指标形成对比。\n- 最后，除了使用参数分布的均值去计算 DIC 中 bias，也可以**使用参数分布的中位数等计算 bias，这提高了 DIC 计算的灵活性**。","metadata":{"id":"3C0C9C395B6B4DC5809799C0C4375F7D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 3. WAIC (Widely applicable information criterion)\n\nWAIC (Widely applicable information criterion) 为“DIC 的升级版”, 由日本统计学家渡边澄夫提出。\n\n相较于 DIC 只考虑了在参数后验分布对数似然的均值，WAIC 考虑了每一个数据点在不同后验参数的影响。\n\n$$\nW A I C=-2\\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right)+2p_{\\mathrm{WAIC}}\n$$","metadata":{"id":"6F6D0291A63D4ECE94CEED0730018CC9","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"![Image Name](https://cdn.kesci.com/upload/image/rl7ysdhz49.png?imageView2/0/w/640/h/640)","metadata":{"id":"9F1C0A3BDB514913A0E8A211B9332019","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"$-2\\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right)$ 为偏差(deviance)。\n\n- WAIC 的偏差与 DIC的偏差 $-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)$ 相似，\n- WAIC 与 DIC 的区别在于增加了 $\\frac{1}{s}\\sum_{j}^{S}$, 代表每个数据点在不同后验参数下似然值的均值。\n- 因为这个特性，WAIC 的偏差有个新名字，lppd (log pointwise predictive density)。\n- lppd 与前面的偏差 (lpd, log predictive density)的区别在于，多了pointwise的步骤：\n  - WAIC 每次选择一个数据点，计算它在所有后验采样上的似然值 $\\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)$，\n  - 再求这些似然值在不同后验参数$\\theta^j$上的平均值 $\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)$，\n  - 最后将不同数据点上的似然值求和，即 $\\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right) = \\sum_{i}^{n} \\log \\bar{L}$。","metadata":{"id":"5638CB26F4B54FCF8704091AC3C8CE06","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"与 DIC 类似的是，矫正项 $p_{\\mathrm{WAIC}}$ 有两种表达形式：\n\n- $p_{\\mathrm{WAIC}}=2 \\sum_{i=1}^{n}\\left(\\log \\left(\\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_{i} \\mid \\theta^{s}\\right)\\right)-\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y_{i} \\mid \\theta^{s}\\right)\\right)$\n- $p_{\\mathrm{WAIC}} = \\sum_{i}^{n}\\left({V}^{s}_{j} \\log p\\left(Y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right)$\n\n其中 $\\sum_{i=1}^{n}\\log \\left(\\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_{i} \\mid \\theta^{s}\\right)\\right)$ 为偏差或 lppd。 $\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y_{i} \\mid \\theta^{s}\\right)$ 为单个数据点$y_{i}$在所有后验参数$\\theta^{s}$下对数似然的均值。","metadata":{"id":"F45A19065FA34EF289214FF836ACB6B4","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 4. LOO-CV (Leave-one-out cross-validation)\n\nWAIC 是非常优秀且常用的指标，其本质是模型对于未知数据的预测能力的**近似**。\n\n另一种与 WAIC 非常类似的近似方法是前文提到的**留一交叉验证法** (Leave-one-out cross-validation, LOO-CV) \n\n$$\nELPD_{LOO-CV} = \\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}_{-i}^{j}\\right)\\right)\n$$","metadata":{"id":"09CC2E898BCB45A09F1E96C9AE55700D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"- ELPD 为 expected log predictive density。\n- $ELPD_{LOO-CV}$ 利用了留一交叉验证的思想，用去除数据点i剩下的数据$y_-i$拟合模型；再回过来用该模型的参数去预测数据点$y_i$。\n- 因此，ELPD 与 WAIC 的偏差非常类似。区别在于，似然中的参数值不是通过所有数据进行拟合的，而是通过去除数据点i剩下的数据拟合得到的参数 $\\theta_{-i}$。\n- 此外，更巧妙的是，由于 $ELPD_{LOO-CV}$ 是直接对非拟合数据$y_i$进行预测，因此不需要再矫正模型。","metadata":{"id":"5D13B623AB9949B89C163B2D1BA20F43","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"在实际操作中，我们通过 `ArViz` 的函数可以很容易的获得 WAIC 和 $ELPD_{LOO-CV}$，我们称为 **LOO** 方法。\n\n此外，由于 $ELPD_{LOO-CV}$ 的计算量也比较大，ArViz 会使用 Pareto Smooth Importance Sampling Leave Once Out Cross Validation (PSIS-LOO-CV) 来近似 $ELPD_{LOO-CV}$。\n\nPSIS-LOO-CV 有两大优势：\n1. 计算速度快，且结果稳健\n2. 提供了丰富的模型诊断指标","metadata":{"id":"37D637751D654D48A7CD20F9400F674D","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"### Model Averaging\n\n前面我们讨论了如何评估单个模型的**拟合优度**与**预测精度**。\n\n但判断模型预测性能的另一个思路是：拟合多个模型，比较不同模型的预测能力。在实践中，我们对不同的模型赋予不同的权重，并组合他们生成一个元模型 (meta-model)，进而进行元预测，以此评估不同模型权重的影响。\n\n这种给不同模型赋予权重的方法称为 模型平均法 Model Averaging。\n常见有三种计算模型权重的方式：\n1. Marginal likelihood\n2. BIC (Bayesian information criterion)\n3. Pseudo Bayesian model averaging","metadata":{"id":"39FC72DAF4474CC78D905C435FEC50B3","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 1. Marginal likelihood\n\n边缘似然或者边际似然 (marginal likelihood) 是贝叶斯公式的分布部分\n- 即 $p(\\theta|data)=\\frac{p(data|\\theta)p(\\theta)}{p(data)}$ 中的 $p(data) = \\int_{\\theta}^{} p(data|\\theta)p(\\theta)d\\theta$ \n- 边缘似然与贝叶斯公式分子部分的似然不同，表达了模型对数据的平均拟合 (Average fit)，因此它可以作为模型选择的指标。*边缘似然越大，说明模型对样本数据解释的越好*。\n- 当比较两个模型时，可以将边缘似然转化为**贝叶斯因子**(Bayes factor)。","metadata":{"id":"0FE6A228F19B4C31AD594CF4ED082F81","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"Bayesian Model Averaging\n\n$$\nw_k = \\frac{e^{-ML_{k}}}{\\sum e^{-ML_{k}}}\n$$\n\n通过边缘似然可以计算 Bayesian Model Averaging。\n- 假设有 k 各模型。\n- k个模型的边缘似然为 $ML_{k}$。\n- k个模型的权重 $w_k$ 为当前模型的边缘似然 $ML_{k}$ 比上 所有模型边缘似然之和 $\\sum e^{-ML_{k}}$。\n","metadata":{"id":"08C576B4445B41C485C0E9D9DEADBBF3","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 2. BIC (Bayesian information criterion)\n\n因为边缘似然计算量巨大，因此我们需要一些快速的计算方式，比如 BIC (Bayesian information criterion)。\n\n$$\nA I C=-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)+2 k\n$$\n\n$$\nB I C=-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right) + 2 k*ln(n)\n$$\n\nBIC 与 AIC 非常类似，区别在于惩罚项 error的不同。\n- BIC 中的 error 在 AIC error 的基础上乘以 ln(n)。\n- 其中，k为模型参数的数量；n为数据的数量。","metadata":{"id":"EAAA77D3448F421CA008D1F155E6301B","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"BIC 的特点：\n- 在公式上与 AIC 高度相似，因此可以用来检验模型拟合优度，其值越小，模型拟合越好。\n- BIC 的 error 往往比 AIC 的更大，即惩罚更大，因此，**BIC 通常会选择简单的模型**。\n- BIC 虽然适用于贝叶斯模型，但是它没有考虑先验的影响。\n- 最重要的是，BIC **是边缘似然的近似**，计算速度比 ML 更快，并且同样也可以被用来计算贝叶斯因子。","metadata":{"id":"349F2AABFA5945FE88BF25ACB3F67A93","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"#### 3. Pseudo Bayesian model averaging\n\nBIC 虽然能近似边缘似然， 但是其 error 只考虑了参数数量和数据数量的复杂性，也没有考虑到先验的影响。\n这样计算的模型权重很可能存在偏差。\n\n为了更高效的计算模型的权重。一种可行的方法是 Pseudo Bayesian model averaging。\n- 即通过 WAIC 与 LOO 来近似边缘似然。\n- 再通过 Bayesian model averaging 公式计算模型权重。\n\n\n$$\nw_i = \\frac{e^{-\\Delta_{i}}}{\\sum_{j}^{k} e^{-\\Delta_{j}}}\n$$\n","metadata":{"id":"2516CB18B09343AA814C477A27A1D475","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"上面的公式与 Bayesian model averaging 的公式 $w_k = \\frac{e^{-ML_{k}}}{\\sum e^{-ML_{k}}}$ 很像。\n区别在于：\n- 通过 WAIC 或者 LOO 在模型中的差值 $\\Delta_{j}$ 替代了 边缘似然 ML。\n- $\\Delta$ 表示的是，第 i 或者 j个模型的 WAIC 与 最优模型的 WAIC 的差值。\n\nPyMC3 与 Arviz 提高了很多关于 Pseudo Bayesian model averaging 计算的方法，之后的实践中，我们将注重于通过 Pseudo Bayesian model averaging 展示模型平均法的作用。","metadata":{"id":"39C347C2BBBF4E3E8F0964A2C90CCE58","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"总结：\n\n模型拟合优度的方法包括：\n- 拟合优度 \n- mse \n- 对数似然\n\n模型预测进度的方法包括：\n- AIC\n- DIC\n- WAIC\n- LOO\n\n模型平均法包括：\n- Bayesian model averaging\n- BIC\n- Pseudo Bayesian model averaging","metadata":{"id":"3ABB9CFB686D42258608205EE662C416","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"markdown","source":"|                    | AIC                                  | DIC                                      | WAIC       | LOOCV           | BIC                                  |\n| ------------------ | ------------------------------------ | ---------------------------------------- | ---------- | --------------- | ------------------------------------ |\n| 适用框架           | 频率论                               | 贝叶斯                                   | 贝叶斯     | 贝叶斯          | 贝叶斯/频率论                        |\n| 偏差（deviance）   | 最大似然参数 $\\theta_mle$ 的对数似然 | 贝叶斯参数均值 $\\bar{\\theta}$ 的对数似然 | LPPD       | $ELPD_{LOO-CV}$ | 最大似然参数 $\\theta_mle$ 的对数似然 |\n| 矫正（correction） | 参数数量                             | 似然的变异                               | 似然的变异 |     由于采用 LOO-CV 思想，因此不需要矫正            | 参数数量+数据数量                    |","metadata":{"id":"B3FEE9871723470887AF0B7F36EBDA08","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true}},{"cell_type":"code","source":"","metadata":{"id":"6EC9A2B1974C4F0980D539EB00A3CBA6","notebookId":"6371c72700125077ce706c07","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}