{"cells":[{"cell_type":"markdown","metadata":{"collapsed":false,"id":"AB2F72934DA24227B3A2E1EF18DAD1C4","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["### 模型的泛化能力：预测准确性 (predictive accuracy)\n","\n","前面提到的诸多模型拟合优度 (Goodness of fit) 指标，只能衡量模型对于当前样本的拟合程度。\n","\n","对于样本外的数据，我们不确定该模型是否具有**泛化能力**，即该模型是否能准确的预测样本外的数据。\n","\n","为了评估模型的**预测准确性 (predictive accuracy)**，我们有以下3种策略：\n","1. 通过新数据对模型进行评估。我们可以收集新的数据，并检验模型的预测能力。\n","2. 从已有样本中拿出一部分数据用来预测。即交叉验证。\n","3. 通过统计方法进行近似。比如使用对交叉验证近似的信息熵指标来评估模型的泛化能力。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"449A531CF85F4E9ABEED728BB22EABF7","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 交叉验证\n","\n","收集新数据来检验模型的预测能力是一种理所当然的直觉。但心理学数据不同于其他学科的数据，它常受到**时间因素**和**抽样**的影响。\n","- 比如，心境可能随着季节变化，因此在不同季节收集到的数据会受到时间的影响。\n","\n","因此，一种更高效的方法是，一次性多收集一些数据，选择其中的一部分作为预测数据。\n","\n","但问题在于，我们选择哪一部分数据作为预测数据呐？或者说，我们该如何有效的对数据进行抽取呐？\n","\n","**交叉验证**的目的就在于：提供不同的抽取预测数据的策略"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"B1CADF6BDCDC42E79918C6A1FC066CBC","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["常见的交叉验证策略：\n","1. 分半交叉验证 (Split-half cross-validation)\n","\t- 分半交叉验证将观测数据对半分成两部分，分别在不同的数据集上拟合模型，并在另外一半数据集上验证模型，最后再对比不同的模型在两份数据集作为验证集时的预测准确度。\n","2. K 折交叉验证 (K-fold cross-validation) \n","\t- K 折交叉验证把数据分成 K 分，其中一份作为训练集，其余的 K-1 分数据集作为验证集，总共重复这个流程 K 次。以 K 次验证结果的均值作为验证标准。\n","3. 留一法交叉验证 (Leave-one-out cross-validation)\n","\t- 留一法交叉验证是 K 折交叉验证的一个特例，当分折的数量等于数据的数量时，K 折留一法便成了留一法交叉验证。留一法交叉验证相较于普通的交叉验证方法，几乎使用了所有数据去训练模型，因此留一法交叉验证的训练模型时的**偏差 (bias) 更小、更鲁棒**，但是又因为验证集只有一个数据点，验证模型的时候**留一法交叉验证的方差 (Variance) 也会更大**。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"268B2411BC6A473E9DE411265BF42AAC","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["K 折交叉验证图示\n","![](https://hub.packtpub.com/wp-content/uploads/2019/05/KFold.png)\n","\n","source: https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"15E81112F633419FA4F9DB96C71AC219","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 信息准则(information criteria)\n","\n","尽管交叉验证存在诸多优势，并且避免了收集数据的潜在问题，但是交叉验证**在认知神经科学里的认知建模领域里的使用并不广泛**，\n","\n","其最主要的原因在于，研究者收集的数据**样本量往往有限**，然而很多计算模型却对数据样本量有所需求，如果使用交叉验证，将数据拆分，那么很有可能导致拟合模型的试次数量不足，使得模型拟合和验证的结果较差，进而导致产生一类错误和二类错误的概率增大。\n","\n","为了解决这些问题，统计学家提出了一种衡量预测准确性的指标，即**信息准则**(information criteria)。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"A3BA6B8A87B3496C9D44EF22D9BA5588","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["信息准则是对统计模型**预测精度**的一种度量。它考虑**模型与数据的匹配程度**，并通过**模型的复杂性**进行矫正(correction to bias)。\n","\n","$信息准则 = deviance + correction$\n","- deviance 为偏差，反应了模型与数据的匹配程度。可以通过对数似然 log likelihood 进行计算。注意 log likelihood 也可以称为 lpd (log predictive density)。\n","- correction 为矫正，与模型的复杂程度相关。模型越复杂时越容易过拟合(overfitting)，因此，矫正项也会越大。\n","\n","可见，信息准则越小，偏差和矫正就越小。因此，**信息准则越小，代表模型的预测性越好**。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"F22BA4CC600F4E28B125358CAD77ACB1","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["常见信息准则有4类：\n","1. AIC (Akaike information criterion)\n","2. DIC (Deviance information criterion)\n","3. WAIC (Widely applicable information criterion)\n","4. LOO-CV (Leave-one-out cross-validation)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"63741BADA01A47B294A0839838677898","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 1. AIC (Akaike information criterion)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"7ABEB248AF6F4249831570CE2472A8D4","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["AIC是最简单的信息准则标准，由日本统计学家赤池弘次 (Hirotugu Akaike) 提出 (Akaike, 1974)，是频率主义统计学里最为经典的模型比较指标之一。\n","\n","其表达式如下：\n","$$\n","A I C= -2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)+2 p_{A I C}\n","$$\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"316727DBDAB14FB6988BEA5AFE02591C","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["$-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$ 为偏差deviance，描述了模型对于当前数据的匹配程度。\n","- 其中，$\\hat{\\theta}_{m l e}$为最大似然法求得的参数值。 \n","- $\\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$ 为对数似然(log likelihood)，也可称为 lpd (log predictive density)。\n","- 偏差deviance为负二倍(-2*)的对数似然值。模型拟合的越好，似然值越大，因此$\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$越大。相应的 $-2\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$ 的值越小。那么 AIC的值也越小。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"7FB883B7C30141078092183CC90662E5","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["$2 p_{A I C}$ 为矫正(correction)。\n","- 其中，$p_{A I C}$ 为模型的参数数量。\n","- 矫正(correction)为参数数量的两倍，描述了模型的复杂程度。模型越复杂，潜在的参数数量可能越多，那么$p_{A I C}$越大。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1969289189F7477D81C4D7AAB053F640","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["需要注意的是 AIC 只考虑了最大似然对应的参数值 $\\hat{\\theta}_{m l e}$, 因此它适用于频率学派模型的评估。而对于贝叶斯学派来说，**由于参数为分布，因此不能使用AIC来评估模型**。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"F6A3B2A44A7E436A82E7203F999F3247","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 2. DIC (Deviance information criterion)\n","\n","为了解决 AIC 无法评估贝叶斯模型。\n","\n","统计学家们提出了\"贝叶斯参数估计版的 AIC\"，即 DIC (Deviance information criterion) 。\n","\n","$$\n","\\mathrm{DIC} = -2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right) +2 p_{DIC}\n","$$"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"1CF12B426CF1487B911DDC8733679EE6","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["$-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)$ 为偏差(deviance)，记为D。\n","- 可以看到，该偏差与AIC中的偏差$-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)$非常相似。\n","- 区别在于，AIC 中的 $\\hat{\\theta}_{m l e}$被替换为$\\bar{\\theta}$。因为贝叶斯框架中，参数不是固定值而是概率分布，因此 $\\bar{\\theta}$ 代表的是参数后验分布的均值 $\\bar{\\theta}$。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"6AD7CD1D11794FA6936075E4C9555E8B","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["![Image Name](https://cdn.kesci.com/upload/image/rl7ys1gvzz.png?imageView2/0/w/640/h/640)"]},{"cell_type":"markdown","metadata":{},"source":["$p_{\\mathrm{DIC}}$ 为矫正(correction). \n","\n","$p_{\\mathrm{DIC}}= 2(\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)-\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y \\mid \\theta^{s}\\right))$\n","- 其中$\\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)$为偏差D。\n","- $\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y \\mid \\theta^{s}\\right)$为所有参数采样所计算到的对数似然的平均值。参数采样包含s个样本，编号为1到s。"]},{"cell_type":"markdown","metadata":{},"source":["相比于AIC的矫正项 $p_{\\mathrm{AIC}}$，$p_{\\mathrm{DIC}}$ 是基于数据的矫正(data-based bias correction)。因为 $p_{\\mathrm{DIC}}$ 考虑了数据在不同参数的对数似然下的影响。\n","\n","$p_{\\mathrm{DIC}}$ 存在另一种表达方式：\n","$$\n","p_{\\mathrm{DIC}}=2\\operatorname{var}(\\log p(y \\mid \\theta))\n","$$\n","- 其中，var表示计算方差。$\\operatorname{var}(\\log p(y \\mid \\theta))$表示所有参数计算所得到的对数似然的方差。\n","- 这种计算方式与第一种计算方式得到的结果一致，并且可以避免 $p_{\\mathrm{DIC}}$ 为负数。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"98138646A7D5462690974645D4036660","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["DIC  是心理学领域最常用的模型评估指标之一。\n","- 与 AIC 只是简单地使用了参数数量作为复杂度的惩罚项不同的是，DIC **利用了 MCMC 采样的参数后验分布去计算模型的有效参数数量**。\n","- 另外 DIC的计算速度与AIC一样很快，这与后面会介绍的其他指标形成对比。\n","- 最后，除了使用参数分布的均值去计算 DIC 中 bias，也可以**使用参数分布的中位数等计算 bias，这提高了 DIC 计算的灵活性**。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"26F33DA106EC494DA0F7A8ABAEE7A02E","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 3. WAIC (Widely applicable information criterion)\n","\n","WAIC (Widely applicable information criterion) 为“DIC 的升级版”, 由日本统计学家渡边澄夫提出。\n","\n","相较于 DIC 只考虑了在参数后验分布对数似然的均值，WAIC 考虑了每一个数据点在不同后验参数的影响。\n","\n","$$\n","W A I C=-2\\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right)+2p_{\\mathrm{WAIC}}\n","$$"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"832D98262DB94193AF874E9F4CA43F2B","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["![Image Name](https://cdn.kesci.com/upload/image/rl7ysdhz49.png?imageView2/0/w/640/h/640)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"826C805F8C7F4C06AFA50144FAE507C3","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["$-2\\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right)$ 为偏差(deviance)。\n","\n","- WAIC 的偏差与 DIC的偏差 $-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\bar{\\theta}\\right)$ 相似，\n","- WAIC 与 DIC 的区别在于增加了 $\\frac{1}{s}\\sum_{j}^{S}$, 代表每个数据点在不同后验参数下似然值的均值。\n","- 因为这个特性，WAIC 的偏差有个新名字，lppd (log pointwise predictive density)。\n","- lppd 与前面的偏差 (lpd, log predictive density)的区别在于，多了pointwise的步骤：\n","  - WAIC 每次选择一个数据点，计算它在所有后验采样上的似然值 $\\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)$，\n","  - 再求这些似然值在不同后验参数$\\theta^j$上的平均值 $\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)$，\n","  - 最后将不同数据点上的似然值求和，即 $\\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right) = \\sum_{i}^{n} \\log \\bar{L}$。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"20285AAC5F27430CBC659F9E7878CFD9","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["与 DIC 类似的是，矫正项 $p_{\\mathrm{WAIC}}$ 有两种表达形式：\n","\n","- $p_{\\mathrm{WAIC}}=2 \\sum_{i=1}^{n}\\left(\\log \\left(\\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_{i} \\mid \\theta^{s}\\right)\\right)-\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y_{i} \\mid \\theta^{s}\\right)\\right)$\n","- $p_{\\mathrm{WAIC}} = \\sum_{i}^{n}\\left({V}^{s}_{j} \\log p\\left(Y_{i} \\mid \\boldsymbol{\\theta}^{j}\\right)\\right)$\n","\n","其中 $\\sum_{i=1}^{n}\\log \\left(\\frac{1}{S} \\sum_{s=1}^{S} p\\left(y_{i} \\mid \\theta^{s}\\right)\\right)$ 为偏差或 lppd。 $\\frac{1}{S} \\sum_{s=1}^{S} \\log p\\left(y_{i} \\mid \\theta^{s}\\right)$ 为单个数据点$y_{i}$在所有后验参数$\\theta^{s}$下对数似然的均值。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"4BB20F8A4C8343D7ACEEFE33934CCB87","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 4. LOO-CV (Leave-one-out cross-validation)\n","\n","WAIC 是非常优秀且常用的指标，其本质是模型对于未知数据的预测能力的**近似**。\n","\n","另一种与 WAIC 非常类似的近似方法是前文提到的**留一交叉验证法** (Leave-one-out cross-validation, LOO-CV) \n","\n","$$\n","ELPD_{LOO-CV} = \\sum_{i}^{n} \\log \\left(\\frac{1}{s} \\sum_{j}^{S} p\\left(y_{i} \\mid \\boldsymbol{\\theta}_{-i}^{j}\\right)\\right)\n","$$"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"D040BC3FBBD146BA9E557E5145D48374","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["- ELPD 为 expected log predictive density。\n","- $ELPD_{LOO-CV}$ 利用了留一交叉验证的思想，用去除数据点i剩下的数据$y_-i$拟合模型；再回过来用该模型的参数去预测数据点$y_i$。\n","- 因此，ELPD 与 WAIC 的偏差非常类似。区别在于，似然中的参数值不是通过所有数据进行拟合的，而是通过去除数据点i剩下的数据拟合得到的参数 $\\theta_{-i}$。\n","- 此外，更巧妙的是，由于 $ELPD_{LOO-CV}$ 是直接对非拟合数据$y_i$进行预测，因此不需要再矫正模型。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"4D961BE79CBD470D9D4F433F03968BFB","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"E05B1ACC376F44319A541C7CA6225850","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["在实际操作中，我们通过 `ArViz` 的函数可以很容易的获得 WAIC 和 $ELPD_{LOO-CV}$，我们称为 **LOO** 方法。\n","\n","此外，由于 $ELPD_{LOO-CV}$ 的计算量也比较大，ArViz 会使用 Pareto Smooth Importance Sampling Leave Once Out Cross Validation (PSIS-LOO-CV) 来近似 $ELPD_{LOO-CV}$。\n","\n","PSIS-LOO-CV 有两大优势：\n","1. 计算速度快，且结果稳健\n","2. 提供了丰富的模型诊断指标"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"BAC3D301FD7A4A4AB3F901150EE94C98","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["### Model Averaging\n","\n","前面我们讨论了如何评估单个模型的**拟合优度**与**预测精度**。\n","\n","但判断模型预测性能的另一个思路是：拟合多个模型，比较不同模型的预测能力。在实践中，我们对不同的模型赋予不同的权重，并组合他们生成一个元模型 (meta-model)，进而进行元预测，以此评估不同模型权重的影响。\n","\n","这种给不同模型赋予权重的方法称为 模型平均法 Model Averaging。\n","常见有三种计算模型权重的方式：\n","1. Marginal likelihood\n","2. BIC (Bayesian information criterion)\n","3. Pseudo Bayesian model averaging"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"2CE5C18A893341EF88BEE954BE7ED75C","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 1. Marginal likelihood\n","\n","边缘似然或者边际似然 (marginal likelihood) 是贝叶斯公式的分布部分\n","- 即 $p(\\theta|data)=\\frac{p(data|\\theta)p(\\theta)}{p(data)}$ 中的 $p(data) = \\int_{\\theta}^{} p(data|\\theta)p(\\theta)d\\theta$ \n","- 边缘似然与贝叶斯公式分子部分的似然不同，表达了模型对数据的平均拟合 (Average fit)，因此它可以作为模型选择的指标。*边缘似然越大，说明模型对样本数据解释的越好*。\n","- 当比较两个模型时，可以将边缘似然转化为**贝叶斯因子**(Bayes factor)。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"9252096431E94EB8B0709D86806D7304","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["Bayesian Model Averaging\n","\n","$$\n","w_k = \\frac{e^{-ML_{k}}}{\\sum e^{-ML_{k}}}\n","$$\n","\n","通过边缘似然可以计算 Bayesian Model Averaging。\n","- 假设有 k 各模型。\n","- k个模型的边缘似然为 $ML_{k}$。\n","- k个模型的权重 $w_k$ 为当前模型的边缘似然 $ML_{k}$ 比上 所有模型边缘似然之和 $\\sum e^{-ML_{k}}$。\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"FB0E8CFCEA9D41648EA1FBD5E38F844F","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 2. BIC (Bayesian information criterion)\n","\n","因为边缘似然计算量巨大，因此我们需要一些快速的计算方式，比如 BIC (Bayesian information criterion)。\n","\n","$$\n","A I C=-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right)+2 k\n","$$\n","\n","$$\n","B I C=-2 \\sum_{i}^{n} \\log p\\left(y_{i} \\mid \\hat{\\theta}_{m l e}\\right) + 2 k*ln(n)\n","$$\n","\n","BIC 与 AIC 非常类似，区别在于惩罚项 error的不同。\n","- BIC 中的 error 在 AIC error 的基础上乘以 ln(n)。\n","- 其中，k为模型参数的数量；n为数据的数量。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"DFFEF5A074ED4E95B4D5A765CFD44CC9","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["BIC 的特点：\n","- 在公式上与 AIC 高度相似，因此可以用来检验模型拟合优度，其值越小，模型拟合越好。\n","- BIC 的 error 往往比 AIC 的更大，即惩罚更大，因此，**BIC 通常会选择简单的模型**。\n","- BIC 虽然适用于贝叶斯模型，但是它没有考虑先验的影响。\n","- 最重要的是，BIC **是边缘似然的近似**，计算速度比 ML 更快，并且同样也可以被用来计算贝叶斯因子。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"3B6D6D3C66DA4A71834A49A9C31DAEF4","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["#### 3. Pseudo Bayesian model averaging\n","\n","BIC 虽然能近似边缘似然， 但是其 error 只考虑了参数数量和数据数量的复杂性，也没有考虑到先验的影响。\n","这样计算的模型权重很可能存在偏差。\n","\n","为了更高效的计算模型的权重。一种可行的方法是 Pseudo Bayesian model averaging。\n","- 即通过 WAIC 与 LOO 来近似边缘似然。\n","- 再通过 Bayesian model averaging 公式计算模型权重。\n","\n","\n","$$\n","w_i = \\frac{e^{-\\Delta_{i}}}{\\sum_{j}^{k} e^{-\\Delta_{j}}}\n","$$\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"9B858DB4D2AC49628342C875DC189A4D","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["上面的公式与 Bayesian model averaging 的公式 $w_k = \\frac{e^{-ML_{k}}}{\\sum e^{-ML_{k}}}$ 很像。\n","区别在于：\n","- 通过 WAIC 或者 LOO 在模型中的差值 $\\Delta_{j}$ 替代了 边缘似然 ML。\n","- $\\Delta$ 表示的是，第 i 或者 j个模型的 WAIC 与 最优模型的 WAIC 的差值。\n","\n","PyMC3 与 Arviz 提高了很多关于 Pseudo Bayesian model averaging 计算的方法，之后的实践中，我们将注重于通过 Pseudo Bayesian model averaging 展示模型平均法的作用。"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"A65A8202204D49F59C88220EC39413DB","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["总结：\n","\n","模型拟合优度的方法包括：\n","- 拟合优度 \n","- mse \n","- 对数似然\n","\n","模型预测进度的方法包括：\n","- AIC\n","- DIC\n","- WAIC\n","- LOO\n","\n","模型平均法包括：\n","- Bayesian model averaging\n","- BIC\n","- Pseudo Bayesian model averaging"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"DBE02B0FAB6140E383E8301C9FF4EEC6","jupyter":{},"notebookId":"636659f4d28b18529a41c592","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":["|                    | AIC                                  | DIC                                      | WAIC       | LOOCV           | BIC                                  |\n","| ------------------ | ------------------------------------ | ---------------------------------------- | ---------- | --------------- | ------------------------------------ |\n","| 适用框架           | 频率论                               | 贝叶斯                                   | 贝叶斯     | 贝叶斯          | 贝叶斯/频率论                        |\n","| 偏差（deviance）   | 最大似然参数 $\\theta_mle$ 的对数似然 | 贝叶斯参数均值 $\\bar{\\theta}$ 的对数似然 | LPPD       | $ELPD_{LOO-CV}$ | 最大似然参数 $\\theta_mle$ 的对数似然 |\n","| 矫正（correction） | 参数数量                             | 似然的变异                               | 似然的变异 |     由于采用 LOO-CV 思想，因此不需要矫正            | 参数数量+数据数量                    |"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('bayesian-analysis-nnupsy': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"5b8a055693dda3df97915f81219c381a2cb70b37ff38c11a66c4761c462a0499"}}},"nbformat":4,"nbformat_minor":2}
